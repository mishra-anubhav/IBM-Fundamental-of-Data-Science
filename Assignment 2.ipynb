{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Assignment 2\nWelcome to Assignment 2. This will be fun. It is the first time you actually access external data from ApacheSpark. \n\n#### You can also submit partial solutions\n\nJust make sure you hit the play button on each cell from top to down. There are three functions you have to implement. Please also make sure than on each change on a function you hit the play button again on the corresponding cell to make it available to the rest of this notebook.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "This notebook is designed to run in a IBM Watson Studio Apache Spark runtime. In case you are running it in an IBM Watson Studio standard runtime or outside Watson Studio, we install Apache Spark in local mode for test purposes only. Please don't use it in production."
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already up-to-date: pip in /opt/conda/envs/Python36/lib/python3.6/site-packages (20.0.2)\r\n"
                }
            ],
            "source": "!pip install --upgrade pip"
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [],
            "source": "if not ('sc' in locals() or 'sc' in globals()):\n    print('It seems you are note running in a IBM Watson Studio Apache Spark Notebook. You might be running in a IBM Watson Studio Default Runtime or outside IBM Waston Studio. Therefore installing local Apache Spark environment for you. Please do not use in Production')\n    \n    from pip import main\n    main(['install', 'pyspark==2.4.5'])\n    \n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\n\n    sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n    \n    spark = SparkSession \\\n        .builder \\\n        .getOrCreate()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "So the function below is used to make it easy for you to create a data frame from a Cloud Object Store data frame using the so called \"DataSource\" which is some sort of a plugin which allows ApacheSpark to use different data sources."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "This is the first function you have to implement. You are passed a dataframe object. We've also registered the dataframe in the ApacheSparkSQL catalog - so you can also issue queries against the \"washing\" table using \"spark.sql()\". Hint: To get an idea about the contents of the catalog you can use: spark.catalog.listTables().\nSo now it's time to implement your first function. You are free to use the dataframe API, SQL or RDD API. In case you want to use the RDD API just obtain the encapsulated RDD using \"df.rdd\". You can test the function by running one of the three last cells of this notebook, but please make sure you run the cells from top to down since some are dependant of each other...\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [],
            "source": "#Please implement a function returning the number of rows in the dataframe\ndef count():\n    x1 = df.count()\n    return x1"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now it's time to implement the second function. Please return an integer containing the number of fields (columns). The most easy way to get this is using the dataframe API. Hint: You might find the dataframe API documentation useful: http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [],
            "source": "def getNumberOfFields():\n    x2 = len(df.columns)\n    return x2\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Finally, please implement a function which returns a (python) list of string values of the field names in this data frame. Hint: Just copy&past doesn't work because the auto-grader will create a random data frame for testing, so please use the data frame API as well. Again, this is the link to the documentation: http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [],
            "source": "def getFieldNames():\n    x3 = df.columns\n    return x3"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now it is time to grab a PARQUET file and create a dataframe out of it. Using SparkSQL you can handle it like a database. "
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-04-19 17:19:14--  https://github.com/IBM/coursera/blob/master/coursera_ds/washing.parquet?raw=true\nResolving github.com (github.com)... 140.82.113.4\nConnecting to github.com (github.com)|140.82.113.4|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://github.com/IBM/skillsnetwork/blob/master/coursera_ds/washing.parquet?raw=true [following]\n--2020-04-19 17:19:14--  https://github.com/IBM/skillsnetwork/blob/master/coursera_ds/washing.parquet?raw=true\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://github.com/IBM/skillsnetwork/raw/master/coursera_ds/washing.parquet [following]\n--2020-04-19 17:19:14--  https://github.com/IBM/skillsnetwork/raw/master/coursera_ds/washing.parquet\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/skillsnetwork/master/coursera_ds/washing.parquet [following]\n--2020-04-19 17:19:14--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/coursera_ds/washing.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 112048 (109K) [application/octet-stream]\nSaving to: \u2018washing.parquet?raw=true\u2019\n\n100%[======================================>] 112,048     --.-K/s   in 0.02s   \n\n2020-04-19 17:19:15 (5.54 MB/s) - \u2018washing.parquet?raw=true\u2019 saved [112048/112048]\n\n"
                }
            ],
            "source": "!wget https://github.com/IBM/coursera/blob/master/coursera_ds/washing.parquet?raw=true\n!mv washing.parquet?raw=true washing.parquet"
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+--------------------+--------------------+-----+--------+----------+---------+--------+-----+-----------+-------------+-------+\n|                 _id|                _rev|count|flowrate|fluidlevel|frequency|hardness|speed|temperature|           ts|voltage|\n+--------------------+--------------------+-----+--------+----------+---------+--------+-----+-----------+-------------+-------+\n|0d86485d0f88d1f9d...|1-57940679fb8a713...|    4|      11|acceptable|     null|      77| null|        100|1547808723923|   null|\n|0d86485d0f88d1f9d...|1-15ff3a0b304d789...|    2|    null|      null|     null|    null| 1046|       null|1547808729917|   null|\n|0d86485d0f88d1f9d...|1-97c2742b68c7b07...|    4|    null|      null|       71|    null| null|       null|1547808731918|    236|\n|0d86485d0f88d1f9d...|1-eefb903dbe45746...|   19|      11|acceptable|     null|      75| null|         86|1547808738999|   null|\n|0d86485d0f88d1f9d...|1-5f68b4c72813c25...|    7|    null|      null|       75|    null| null|       null|1547808740927|    235|\n|0d86485d0f88d1f9d...|1-cd4b6c57ddbe77e...|    5|    null|      null|     null|    null| 1014|       null|1547808744923|   null|\n|0d86485d0f88d1f9d...|1-a35b25b5bf43aaf...|   32|      11|acceptable|     null|      73| null|         84|1547808752028|   null|\n|0d86485d0f88d1f9d...|1-b717f7289a8476d...|   48|      11|acceptable|     null|      79| null|         84|1547808768065|   null|\n|0d86485d0f88d1f9d...|1-c2f1f8fcf178b2f...|   18|    null|      null|       73|    null| null|       null|1547808773944|    228|\n|0d86485d0f88d1f9d...|1-15033dd9eebb4a8...|   59|      11|acceptable|     null|      72| null|         96|1547808779093|   null|\n|0d86485d0f88d1f9d...|1-753dae825f9a6c2...|   62|      11|acceptable|     null|      73| null|         88|1547808782113|   null|\n|0d86485d0f88d1f9d...|1-b168089f44f03f0...|   13|    null|      null|     null|    null| 1097|       null|1547808784940|   null|\n|0d86485d0f88d1f9d...|1-403b687c6be0dea...|   23|    null|      null|       80|    null| null|       null|1547808788955|    236|\n|0d86485d0f88d1f9d...|1-195551e0455a24b...|   72|      11|acceptable|     null|      77| null|         87|1547808792134|   null|\n|0d86485d0f88d1f9d...|1-060a39fc6c2ddee...|   26|    null|      null|       62|    null| null|       null|1547808797959|    233|\n|0d86485d0f88d1f9d...|1-2234514bffee465...|   27|    null|      null|       61|    null| null|       null|1547808800960|    226|\n|0d86485d0f88d1f9d...|1-4265898bb401db0...|   82|      11|acceptable|     null|      79| null|         96|1547808802154|   null|\n|0d86485d0f88d1f9d...|1-2fbf7ca9a0425a0...|   94|      11|acceptable|     null|      73| null|         90|1547808814186|   null|\n|0d86485d0f88d1f9d...|1-203c0ee6d7fbd21...|   97|      11|acceptable|     null|      77| null|         88|1547808817190|   null|\n|0d86485d0f88d1f9d...|1-47e1965db94fcab...|  104|      11|acceptable|     null|      75| null|         80|1547808824198|   null|\n+--------------------+--------------------+-----+--------+----------+---------+--------+-----+-----------+-------------+-------+\nonly showing top 20 rows\n\n"
                }
            ],
            "source": "df = spark.read.parquet('washing.parquet')\ndf.createOrReplaceTempView('washing')\ndf.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The following cell can be used to test your count function"
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "2058\n"
                }
            ],
            "source": "cnt = None\nnof = None\nfn = None\n\ncnt = count()\nprint(cnt)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The following cell can be used to test your getNumberOfFields function"
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "11\n"
                }
            ],
            "source": "nof = getNumberOfFields()\nprint(nof)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The following cell can be used to test your getFieldNames function"
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "['_id', '_rev', 'count', 'flowrate', 'fluidlevel', 'frequency', 'hardness', 'speed', 'temperature', 'ts', 'voltage']\n"
                }
            ],
            "source": "fn = getFieldNames()\nprint(fn)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Congratulations, you are done. So please submit your solutions to the grader now.\n\n#\u00a0Start of Assignment-Submission\n\nThe first thing we need to do is to install a little helper library for submitting the solutions to the coursera grader:\n"
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2020-04-19 17:19:34--  https://raw.githubusercontent.com/IBM/coursera/master/rklib.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2540 (2.5K) [text/plain]\nSaving to: \u2018rklib.py\u2019\n\n100%[======================================>] 2,540       --.-K/s   in 0s      \n\n2020-04-19 17:19:34 (38.8 MB/s) - \u2018rklib.py\u2019 saved [2540/2540]\n\n"
                }
            ],
            "source": "!rm -f rklib.py\n!wget https://raw.githubusercontent.com/IBM/coursera/master/rklib.py"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now it\u2019s time to submit first solution. Please make sure that the token variable contains a valid submission token. You can obtain it from the coursera web page of the course using the grader section of this assignment.\n\nPlease specify you email address you are using with cousera as well.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 58,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Submission successful, please check on the coursera grader page for the status\n-------------------------\n{\"elements\":[{\"itemId\":\"7Yp62\",\"id\":\"sUpST4RAEeawAApvKZgcCQ~7Yp62~861KNYJhEeqr0RIltKZ8JQ\",\"courseId\":\"sUpST4RAEeawAApvKZgcCQ\"}],\"paging\":{},\"linked\":{}}\n-------------------------\n"
                }
            ],
            "source": "from rklib import submit, submitAll\nimport json\n\nkey = \"SVDiVSHNEeiDqw70MIp2vA\"\n\nif type(23) != type(cnt):\n    raise ValueError('Please make sure that \"cnt\" is a number')\n    \nif type(23) != type(nof):\n    raise ValueError('Please make sure that \"nof\" is a number')\n\nif type([]) != type(fn):\n    raise ValueError('Please make sure that \"fn\" is a list')\n\nemail = \"anubhavmishra271@gmail.com\"\ntoken = \"zIqQlLYvGkSNlri7\"\n\nparts_data = {}\nparts_data[\"2FjQw\"] = json.dumps(cnt)\nparts_data[\"j8gMs\"] = json.dumps(nof)\nparts_data[\"xaauC\"] = json.dumps(fn)\n\n\nsubmitAll(email, token, key, parts_data)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}